{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8a2b8ca619cdb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# COVID-19 Chest X-Ray Database - Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57458cbe",
   "metadata": {},
   "source": [
    "## CNN Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 4\n",
    "IMAGE_SIZE = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ff996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Check project name and other values\n",
    "# Initialize wandb run | 'wandb login' terminal\n",
    "wandb_run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"pretrained_model\": \"RestNet-50\",\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"criterion\": \"Cross entropy loss\",\n",
    "        \"dataset\": \"COVID-19 Chest X-Ray Database\",\n",
    "        \"epochs\": 20\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb090882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8643cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50, freeze early layers\n",
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get number of input features from the original FC layer\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "# Define new classifier head\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_features, 128),  # Example hidden layer\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(128, NUMBER_OF_CLASSES)  # Output layer with your class count\n",
    ")\n",
    "\n",
    "# Combine model and classifier\n",
    "full_model = torch.nn.Sequential(model, classifier)\n",
    "full_model.to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer (replace with your learning rate if needed)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Move model to chosen device\n",
    "full_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, epoch, logs):\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Early stopping triggered after {self.patience} epochs with no improvement.\")\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (your data preprocessing and dataset creation)\n",
    "\n",
    "# Create DataLoaders (replace with your data loaders)\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create data loaders\n",
    "train_loader = None\n",
    "val_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ee339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    # Training phase\n",
    "    for data, target in train_loader:\n",
    "        # Move data and target to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass, calculate loss\n",
    "        output = full_model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagation, update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            # Move data and target to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = full_model(data)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_loss += criterion(output, target).item()\n",
    "            # TODO: Calculate metrics\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    # Log validation, and metrics\n",
    "    if wandb_run is not None:\n",
    "        # TODO: Add metrics\n",
    "        wandb_run.log({\"val_loss\": val_loss})\n",
    "        \n",
    "    if not early_stopping(epoch, logs={'val_loss': val_loss}): break\n",
    "\n",
    "# Finish Wandb run\n",
    "if wandb_run is not None:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422976fb",
   "metadata": {},
   "source": [
    "### Raw Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316916c",
   "metadata": {},
   "source": [
    "### Bilateral Filtered Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52631519cd84600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T06:09:57.670499Z",
     "start_time": "2024-04-29T06:09:57.665628Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(name, classifier, params, cmap):\n",
    "    print(f\"\\n- {name}\")\n",
    "    \n",
    "    model = classifier(**params)    \n",
    "    y_pred, y_pred_proba = fit_and_predict(model, X_train, X_test, y_train, y_test)\n",
    "    metrics, cm = get_metrics(y_test, y_pred, y_pred_proba)\n",
    "    \n",
    "    plot_confusion_matrix(cm, cmap)\n",
    "    \n",
    "    metrics_to_print = metrics.copy()\n",
    "    metrics_to_print.pop('TPR')\n",
    "    metrics_to_print.pop('FPR')\n",
    "    \n",
    "    for metric, value in metrics_to_print.items():\n",
    "            print(f\"    - {metric}: {value}\")\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b446303e9610a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979826e88f66f03a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T06:09:57.689788Z",
     "start_time": "2024-04-29T06:09:57.677506Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "wineDF = pd.read_csv(\"./data/winequality-red.csv\")\n",
    "\n",
    "wineDF['label'] = wineDF['quality'].apply(lambda x: 1 if x > 6 else 0)\n",
    "wineDF.drop('quality', axis=1)\n",
    "\n",
    "wine_y = wineDF['label'].values\n",
    "wine_X = wineDF.drop('label', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11eaedfd98e6409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T06:09:57.695932Z",
     "start_time": "2024-04-29T06:09:57.690794Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardize dataset\n",
    "scaler = StandardScaler()\n",
    "wine_X = scaler.fit_transform(wine_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb87b6d52742ad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T06:09:57.709121Z",
     "start_time": "2024-04-29T06:09:57.696925Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtained from grid search\n",
    "models['Logistic Regression']['params'] = {'C': 0.046415888336127774, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "models['Decision Tree']['params'] = {'max_depth': None, 'criterion': 'entropy'}\n",
    "models['K-Nearest Neighbors']['params'] = {'n_neighbors': 3}\n",
    "models['Neural Network']['params'] = {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate_init': 0.001, 'max_iter': 200, 'solver': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf40d14ac7035bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T06:10:05.609988Z",
     "start_time": "2024-04-29T06:09:57.711109Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross validate\n",
    "all_metrics = {}\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nSplit {i+1}:\")\n",
    "    \n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(wine_X, wine_y, test_size=0.2, stratify=wine_y)\n",
    "    \n",
    "    for key, model in models.items():\n",
    "        metrics = run_model(key, model['classifier'], model['params'], plt.cm.Blues)\n",
    "        all_metrics[key] = metrics\n",
    "        \n",
    "plt.figure()\n",
    "\n",
    "for name, metrics in all_metrics.items():\n",
    "        plt.plot(metrics['FPR'],metrics['TPR'], linestyle='-', color=models[name]['color'], label=name)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Multiple Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7f49cbcfce132",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62d99f84b11ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T06:10:05.624944Z",
     "start_time": "2024-04-29T06:10:05.612975Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "heartDF = pd.read_csv(\"./data/heart_statlog_cleveland_hungary_final.csv\")\n",
    "\n",
    "heart_y = heartDF['target'].values\n",
    "heart_X = heartDF.drop('target', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ffc0e1d55a5b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T06:10:05.635609Z",
     "start_time": "2024-04-29T06:10:05.629930Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardize dataset\n",
    "scaler = StandardScaler()\n",
    "heart_X = scaler.fit_transform(heart_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a456dfc00ec36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T06:10:05.653034Z",
     "start_time": "2024-04-29T06:10:05.637601Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtained from grid search\n",
    "models['Logistic Regression']['params'] = {'C': 0.046415888336127774, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "models['Decision Tree']['params'] = {'max_depth': 20, 'criterion': 'entropy'}\n",
    "models['K-Nearest Neighbors']['params'] = {'n_neighbors': 10}\n",
    "models['Neural Network']['params'] = {'activation': 'relu', 'hidden_layer_sizes': (100,), 'max_iter': 300, 'solver': 'lbfgs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d51f830895fab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T06:10:11.200944Z",
     "start_time": "2024-04-29T06:10:05.656024Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross validate\n",
    "all_metrics = {}\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nSplit {i+1}:\")\n",
    "    \n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(heart_X, heart_y, test_size=0.2, stratify=heart_y)\n",
    "    \n",
    "    for key, model in models.items():\n",
    "        metrics = run_model(key, model['classifier'], model['params'], plt.cm.Oranges)\n",
    "        all_metrics[key] = metrics\n",
    "        \n",
    "plt.figure()\n",
    "\n",
    "for name, metrics in all_metrics.items():\n",
    "        plt.plot(metrics['FPR'],metrics['TPR'], linestyle='-', color=models[name]['color'], label=name)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Multiple Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
